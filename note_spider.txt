r = requests.get(url)
Request对象
Response对象
request.get(url,params=None,**kwargs)
Response对象的属性
r.status_code    HTTP请求的返回状态，200代表连接成功，404表示失败
r.text			HTTP响应内容的字符串形式，即，url对应的页面内容
r.encoding		从HTTPheader中猜测的响应内容编码方式
r.apparent_encoding		从内容中分析出来的响应内容编码方式（备选编码方式）
r.content			HTTP响应内容的二进制形式
Ruquest库异常
request.ConnectionError  网络链接异常，如DNS查询失败、拒绝连接等
request.HTTPError	HTTP错误异常
request.URLRequired		URL缺失异常
request.ToomanyRedirects	超过最大重定向次数，产生重定向异常
request.ConnectTimeout	连接远程服务器超时异常
request.Timeout		请求URL超时，产生超时异常

r.raise_for_status()		如果不是200，产生异常reqests.HTTPError

通用代码框架
import request

def getHTMLText(url):
	try:
		r = requests.get(url,timeout=30)
		r.raise_for_status()
		r.encoding = r.apparent_encoding
		return r.text
	except:
		return "产生异常"
		
if __name__ == "__main__":
	url = "http://www.baidu.com"
	print(getHTMLText(url))
	
	
Requests库的7个主要方法
requests.request()	构造一个一个请求，支撑以下各方法的基础方法
requests.get()		获取HTML网页的主要方法，对应HTTP的GET-请求获取url位置的资源
requests.head()		获取HTML网页头信息的方法，对应HTTP的HEAD-请求url位置资源的响应消息报告，即获取该资源的头部信息
requests.post()		向HTML网页提交POST请求的方法，对应于HTTP的POST-请求url位置的资源后附加新的数据
requests.put()		向HTML网页提交PUT请求的方法，对应于HTT{的PUT-请求url位置储存一个资源，覆盖url位置的资源
requests.patch()	向HTML网页提交布局修改请求，对应于HTTP的PATCH-请求局部跟新url位置的资源，即改变该处资源的部分内容
requests.delete()	向HTML页面提交删除请求，对应于HTTP的DELETE-请求删除url位置储存的资源

HTTP协议
URL格式  http://host[:port][path]
host:合法的Internet主机域名或IP地址
port：端口号，缺省端口为80
path：请求资源的路径

requests.request(method,url,**kwargs)
method:请求方式7种
GET 
HEAD
POST
PUT
PATCH
delete
OPTIONS

**kwargs:控制访问的参数，均为可选项
1、params:字典胡字节序列，作为参数怎加到url
2、data：字典、字节序列或文件对象，作为Request的内容
3、json：JSON格式的数据，作为Request的内容
4、headers：字典，HTTP定制头
5、cookies：字典或cookieJar，Request中的cookie
6、auth：元组，支持HTTP认证功能
7、files：字典类型，传输文件
8、timeout:设定超时时间，秒为单位
9、proxies：字典类型，设定访问代理服务器，可以增加登陆认证
10、allow_redirects:True/False,默认为True，重定向开关
11、stream：True/False，默认为True，获取内容立即下载开关
12、verify:True/False,默认为True，认证SSL证书开关
13、cert：本地SSL证书路径

使用方法
requests.get(url,params=None,**kwargs)------最常用
requests.head(uel,**kwargs)----最常用
requests.post(url,data=None,json=None,**kwargs)
requests.put(url,data=None,**kwargs)
requests.patch(url,data=None,**kwargs)
requests.delete(url,**kwargs)

robots协议含义
*代表所有，/代表根目录

Robots协议的使用
网络爬虫：自动

爬取图片全代码(二进制数据均可）
import requests
import os
url = "http://example.com/abcd.jpg"
root = "D://pics//"
path = root +url.split('/')[-1]
try:
	if not os.path.exists(root):
		os.madir(root)		# 创建根目录
	if not os.path.exits(path):
		r = request.get(rul) # 创建爬取链接
		with open(path,'wb') as f:
		f.close()
		print("文件保存成功“)
	else:
		print("文件已存在“)
except:
	print(“爬取失败”)
	
IP地址查询全代码
import requests
url = "http://m.ip138.com/ip.asp?ip="
try:
	r = requests.get(url+'202.204.80.112')
	r.raise_for_status()
	r.encoding = r.apparent_encoding
	print(r.text[-500:])
except:
	print("爬取失败")

Beautiful Soup

from bs4 import BeautifulSoup
soup = BeautifulSoup('<p>data</p>','html.parser')

BeautifulSoup基本元素
